---
title: Security in the Vibe Coded Age - Hacking Series.so
visibility: public
---

WORK IN PROGRESS.

Building a startup is incredibly hard, especially in the crowded space of consumer social. Despite this, the founders of [Series.so](https://series.so) unveiled themselves from stealth after
raising an astonishing $3M from well-known venture firms and angels. Their core product is their AI social network which users can interface with in iMessage through a
conversational agent. In addition to their network, Series has cultivated hundreds of thousands of eyes on the team through marketing, an
[intern challenge](https://www.linkedin.com/posts/nathaneo-johnson-86aa4a253_introducing-the-series-a-2-week-reality-activity-7344360830127292416-t92R), and
charismatic co-founders who attract college students to use their product.

However, building in public with the data of tens of thousands of college students is a privilege that comes with major responsibilities, especially regarding
application security. In this blog post, I outline two security vulnerabilities I discovered in Series.so's application, the timeline of events, my motivations,
and suggestions for building in the era of AI coding and AI agents

## The Vulnerabilities

Series' frontend is a NextJs application hosted on Vercel, and uses NextAuth to provide Google OAuth sign-in. I began security research after noticing that Series requires you to
provide read access to your Google Calendar to sign up. My calendar is personal, and therefore I am scrutinous of any service that I give access to this data. I was particularly concerned
at how I could view my own user ID and user data in the Chrome console, signaling bad practice for data protection.

I began investigating the network requests of the application. I noticed that on certian pages of the frontend, images were served to the client from an RSC. However, on other pages throughout
the onboarding flow, images were hosted from an external url. This domain ended in `.web.app`, indicating something deplyoed on Google Cloud Run or using Firebase hosting. After heading
to the index route of the api, I noticed a format indicating a FastAPI-style backend. I tested various routes, but I guessed that they used SwaggerUI. Testing the `/openapi.json` route confirmed my
suspicions, and then heading to `/docs` displayed their full backend API.

The second vulnerability I discovered was after they had patched the first vulnerability (see the [timeline](#appendix---timeline-of-events) below). Any user with an authentication token could query
any user's profile by user id, which included both their personally identifying information (PII), all of their connections, and every calendar event that every one of their connections has.

## Scope of the Vulnerability

If a malicious actor were to have had access to this API, they could have performed a concerning number of actions from the API routes exposed. This includes:

- Determine if a user exists by email or phone number
- Create a new user
- Get a user's information by `user_id`, which returns their user object, including PII such as their name, email, phone number,
  age, elo, AI score, archetype, all users they are connected to, data from their LinkedIn profile, gender, and race.
- Reconstruct the social network - because one can search up users by email and users by ID, one could reconstruct Series' social graph through a breadth-first search.
- Upload profile photos to their cloud storage bucket, and modify users' profile photos by ID.

## Concerning Practices

There are a variety of actions that Series performs without notifying the user:

- **LinkedIn scraping**: For the route called "augment user", the Swagger docs state that this function "augment(s) user data with LinkedIn information and create archetype users from network."
  The docs explicitly state to "search and scrape LinkedIn for the user, store in metadata."
- **Gender and Race detection**: For the route called "analyze face endpoint", the documentation states that they "analyze a profile photo for face detection and metadata extraction." The
  OpenAPI schema for `picture_data` contains a field for gender and race, along wtih a confidence score for each field.

Having these features are a major violation of privacy. I believe the user should either be notified that this happens or should be opt-in, but not mentioning that this happens is illegal
in certain states. The [California Privacy Rights Act](https://www.caprivacy.org/cpra-exec-summary/) states that businesses must give specific notice and allow you to restrict or
opt-out for "Sensitive Personal Information", such as race or biometrics.

## Suggestions for improvement

Firstly, authentication is hard; however, this is _not an excuse_ for having unauthenticated endpoints that expose PII. For apps similar to Series performing AI workloads on web applications and
containing sensitive user data, I suggest:

- Use open-source authentication providers like [NextAuth](https://next-auth.js.org/) and [BetterAuth](https://www.better-auth.com/) or managed services like [Clerk](https://clerk.com)
  and [Auth0](https://auth0.com).
- Unless aboslutely necessary, try to keep your application logic tightly coupled to your authentication system. For example, having a NextJs application with NextAuth and then also managing
  authenticating users on another external FastAPI server is _not trivial_. The workloads series ran would certainly run in a Vercel function, and therefore ensuring only authenticated users can
  access these endpoints is as easy as an `await auth()` call.
- When exposing endpoints for customer-facing AI agents that display information or take actions on behalf of the user (tool calls, MCPs, chat apps), put strong guardrails in front of your agents.
  This includes detecting prompt injection, having a robust eval suite, using control theory, and intense observability and tracing of all actions an LLM performs.

4. Concerning things that Series.so does
   - facial recognition - they then scan your face to detect your race and gender with a confidence score
   - they have an agent scrape linkedin to get more information about you
   - There is no way to delete your data. THey request sensitive scopes by being able to read your calendar, thus they must demonstrate the ability to delete user data
     which you currently cannot.
5. Suggestions for improvement
   - The frontend web app is deployed on vercel but their api was hosted elsewhere on google cloud run. Unless you absolutely need workloads to run elsewhere,
     it's likely that Vercel, Render, Netlify, Railway, or other providers will be enough for your use case. Therefore, keep everything in one place, authenticate your
     /api routes, and it's very easy!
6. Reflections on security for AI agents, vibe-coded apps, and LLM-as-api apps.
   - AI Agents - anything that could potentially exfiltrate PII must be absolutely locked down. Run evals, add a llm-as-a-judge, or don't even do it at all.
   - vibe coded apps - i understand it's fun to move fast be bold be proud break things and be young founders but.... don't forget the basics.
   - LLM-as-apis - seems like apps are functioning more and more as frontend → LLM → backend. You have to lock down the LLM layer if it's doing tool calls/MCP.
     use example of neon having the tool `execute_sql()` for example. does this not ring alarm bells???

## Appendix - Timeline of events

### Vulnerability 1 - 7/11/25

7/11/25

- **5:22PM:** I began investigating Series.
- **5:35PM:** I demonstrated full access to their backend API. This was publically available on the internet, and finding it was a matter of following network requests. No additional tools or techniques
  were used.
- **5:41-5:58PM:** I reached out via LinkedIn to one of the co-fouders, and on X to another team member.
- **6:54PM:** After no response through social media, I contacted the founders over email.
- **7:00PM:** First contact with the team over email.
- **7:06PM:** I called the team, responsibly disclosed the vulnerability, and suggested improvements to secure their application. I was told I will be contacted by the team once fixed.
- **~10:00PM:** Vulnerability fixed. This was not communicated to me.

### Vulnerability 2 - 7/12/25-26

7/12/25

- **3:12PM:** I verified that the original vulnerability is still fixed.
- **5:18PM:** Found a new vulnerability of an exposed endpoint. Although requiring authentication, it would allow any user to query any other user's data.
- **5:23PM:** I reached out via email again to the founders.
- **6:15PM:** After no response, I texted a co-founder.
- **6:24PM:** I called a co-founder, responsibly disclosed the other vulnerability, and suggested improvements. I was told I will be contacted by the team once fixed.

7/13/25

- **3:54PM**: I had heard no response from the team, checked if the vulnerability was fixed, and it was not. I then reached out via text again.

_incomplete - they haven't fixed this one yet_.

---
title: Security in the Vibe Coding Age - Hacking Series.so
visibility: public
---

# Series.so Hack

> **TL/DR**: Series' backend API was publicly available to any unauthenticated user, allowing anyone to fully re-create their entire social graph. This includes names, emails, phone numbers, all of their connections, and every calendar event they have with mutuals. Moreover, they scrape your LinkedIn profile and scan your face to detect your race and gender. The team seemed unconcerned by this when I responsibly disclosed the vulnerabilities; they fixed the first vulnerability in 3 hours, and took over **[NUM_DAYS]** days to fix the second vulnerability while simultaneously posting on LinkedIn about their product's userbase growth. I did this out of curiosity and my passion for AI safety, and I'm sharing my findings to help the team improve their security.

Building a startup is hard, especially in the crowded consumer social space. Despite this, the founders of [Series.so](https://series.so) unveiled themselves from stealth after
raising an astonishing $3M from well-known venture firms and angels. Their core product is their AI social network which users can interface with via iMessage, talking to their
conversational agent. In addition to their network, Series has cultivated hundreds of thousands of eyes on the team through marketing, an
[intern challenge](https://www.linkedin.com/posts/nathaneo-johnson-86aa4a253_introducing-the-series-a-2-week-reality-activity-7344360830127292416-t92R), and
charismatic co-founders who attract college students to use their product. However, building in public with the data of tens of thousands of college students is a privilege that comes with _major_ responsibilities, especially regarding
application security... and they left their entire backend API publically available.

## Finding the Vulnerabilities

I began security research after noticing that Series requires you to
provide read access to your Google Calendar to sign up. My calendar is personal, so I am scrutinous of any service that requests this data. I noticed that I could see my own `user_id` and user data in the Chrome console, and it is also given to you as your referral code after completing the onboarding flow.

I then traced the network requests of the application. Images were sometimes loaded server-side from an RSC, but throughout the onbaording flow, I noticed they were loaded from an external API with domain `*.web.app`, indicating some GCR backend. Following that endpoint, I went to the index route, noticed a FastAPI-style response. Then, I guessed that they were using SwaggerUI, went to `/docs`, and this took me to the full SwaggerUI docs for their completely unauthenticated backend. This vulnerability required no hacking skills, just some curiosity and a bit of poking around!

The second vulnerability I discovered was after they had patched the first vulnerability (see the [timeline](#appendix---timeline-of-events) below). Any user with an authentication token could query
_any_ user's profile by `user_id`, which included personally identifying information (PII), all of their connections, and shared calendar events.

## Scope of the Vulnerability

If a malicious actor had access to this API, they could have:

- Determine if a user exists by email or phone number.
- Create a new user.
- Get a user's information by `user_id`, which returns their user object, including PII such as their name, email, phone number,
  age, all users they are connected to, data from their LinkedIn profile, gender, and race (see [concerning practices](#concerning-practices)).
- Since one can search up users by email and users by `user_id`, one could **reconstruct Series' entire social** graph through a breadth-first search.
- Upload profile photos to their cloud storage bucket, and modify users' profile photos by `user_id`.

This is _shockingly_ bad.

## Concerning Practices

There are a variety of actions that Series performs without notifying the user:

- **Gender and Race detection**: For the route called "analyze face endpoint", the API analyzes the user's profile photo for face detection and metadata extraction. The
  [object for `picture_data`](https://gist.github.com/charliemeyer2000/2114dca872fa1903b349be670c48eb25) contains a field for gender and race, along with a confidence score for each field.
  ![gender and race detection](/analyze-face.png)
- **LinkedIn scraping**: For the route called "augment user", the API scrapes the user's LinkedIn for data.
  ![LinkedIn scraping](/linkedin-scrape.png)
- **Calendar event downloading**: Series stores all of your calendar events, and with the second vulnerability, you can see all of the calendar events you share with your connections.

Series' privacy policy states explicitly enumerates [various methods](https://www.series.so/privacy#:~:text=We%20collect%20personal%20information%20from%20you%20in%20the%20following%20ways%3A) they use to collect data about you, but they _do not state_ that they detect gender/race nor scrape LinkedIn. This is a major breach of privacy and a misrepresentation of their privacy policy, and also is illegal in some states (in California, the [California Privacy Rights Act](https://www.caprivacy.org/cpra-exec-summary/) states that businesses must give specific notice and allow you to restrict or opt-out for "Sensitive Personal Information", such as race or biometrics).

## Suggestions for improvement

Firstly, authentication is hard; however, this is _not an excuse_ for having unauthenticated endpoints that expose PII. For apps similar to Series performing AI workloads on web applications and
containing sensitive user data, I suggest:

- Use open-source authentication providers like [NextAuth](https://next-auth.js.org/) and [BetterAuth](https://www.better-auth.com/) or managed services like [Clerk](https://clerk.com)
  and [Auth0](https://auth0.com).
- Unless absolutely necessary, try to keep your application logic tightly coupled to your authentication system. For example, having a next.js application with NextAuth and then also managing
  authenticating users on another external FastAPI server is _not trivial_. The workloads Series ran would certainly run in a Vercel function, and therefore ensuring only authenticated users can access these endpoints is as [easy as an `await auth()` call](https://authjs.dev/getting-started/session-management/get-session).
- When exposing endpoints for customer-facing AI agents that display information or take actions on behalf of the user (tool calls, MCPs, chat apps), put strong guardrails in front of your APIs. This includes detecting prompt injection, having a robust eval suite, using control theory, and intense observability and tracing of all actions an LLM performs.

## Reflections

In the era of vibe-coded applications or AI-driven development, it's incredibly tempting to iterate quickly and test an MVP as soon as possible, yet it seems like we're taking
"move fast and break things" too literally. It's dangerously easy to just click "accept all" from Cursor agent and ship horrifyingly insecure code. Although AI removes a majority of the drudge work from programming, it does not remove the necessity to think about system design, understand business requirements, and create maintainable and secured systems. _AI-generated code should be "untrusted by default,"_ and it is up to the developer to review and test the code before shipping it to production. You own what you ship.

Secondly, many applications are moving to a new architecture:

![LLM-as-API](/frontend-llm-db.png)

_I don't need to explain to you why this is a bad idea_. If you're putting an MCP in front of your database and have a tool called `execute_sql()`, don't be surprised when you have a [Bobby Tables &trade;](https://xkcd.com/327/) incident. I'm not joking—the Neon MCP server [literally has this](https://github.com/neondatabase-labs/mcp-server-neon/blob/main/src/tools/tools.ts#L85)—sensitive customer data is one prompt injection away. Add a layer of abstraction between your MCP/LLM tool calls and your core application logic, detect prompt injection with a [classifier](https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2?text=I+like+you.+I+love+you), add an LLM-as-a-judge, obsess over observability, trace to your agents, and eval, eval, eval! With Series' iMessage agent or _any_ company's customer-facing AI agents, adding layered security is a must.

Finally, I want to clarify _why_ I did this—pure curiosity. I care deeply about AI safety and application safety, and poking around websites is just a fun hobby of mine. I disclosed this vulnerability and am writing this because I want to see them succeed, but for the safety of all of the users & their data, catching this early and fixing it is paramount. The magnitude of the damage could have been much worse.

## Conclusion

Thanks to the many people who have supported me through this process of responsible disclosure and writing this report. Your feedback and advice has been invaluable.

At the time of writing this, I have yet to receive a bug bounty from the Series team.

> "Any fool can use a computer. Many do." - [Martin Fowler](https://en.wikipedia.org/wiki/Martin_Fowler)

---

## Appendix

## Timeline of Events

### Vulnerability 1 - 7/11/25

7/11/25

- **5:22PM:** I began investigating Series.
- **5:35PM:** I demonstrated fully unauthenticated access to their backend API.
  were used.
- **5:41-5:58PM:** I reached out via LinkedIn to one of the co-founders, and on X to another team member.
- **6:54PM:** After no response through social media, I contacted the founders over email.
- **7:00PM:** First contact with the team over email.
- **7:06PM:** I called the team, responsibly disclosed the vulnerability, and suggested improvements to secure their application. I was told I will be contacted by the team once fixed.
- **~10:00PM:** Vulnerability fixed. This was not communicated to me.

### Vulnerability 2 - 7/12/25-26

7/12/25

- **3:12PM:** I verified that the original vulnerability is still fixed.
- **5:18PM:** Found a new vulnerability of an exposed endpoint.
- **5:23PM:** I reached out via email again to the founders.
- **6:15PM:** After no response, I texted a co-founder.
- **6:24PM:** I called a co-founder, responsibly disclosed the new vulnerability, and proposed a fix which would require making this API call a server action or deleting the route entirely—both of which are incredibly easy hotfixes. I was told I will be contacted by the team once fixed.

7/13/25

- **3:54PM**: I had heard no response from the team, checked if the vulnerability was fixed, and it was not. I then reached out via text again.
- **5:47PM**: I received a response from the team stating they have been working all day and are almost done.

7/14/25:

- **~7:00AM**: The founders [posted on LinkedIn](https://www.linkedin.com/posts/nathaneo-johnson-86aa4a253_over-75000-profiles-were-made-on-series-activity-7350510699858518017-aQYj?utm_source=share&utm_medium=member_desktop&rcm=ACoAAEDpYpAB1feySUyx1V2ZZ-Z7ePb5CStO5W8) about their product, and the second vulenerability is still not fixed.

_TODO: incomplete - they haven't fixed this one yet_.

## Examples

- For vulnerability 1, [here's](https://series-swagger-docs.vercel.app/) a snapshot of the SwaggerUI docs. I didn't download the `openapi.json`, so it's a bit non-functional (sorry). I have redacted the actual routes to protect the team.
- For vulnerability 2, I can see shared calendar events of my connections (blurred for privacy) along with all of their data (names, emails, connections).
  ![calendar events](/calendar-events.png)

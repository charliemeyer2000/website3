---
title: Security in the Vibe Coding Age - Hacking Series.so
visibility: public
---

WORK IN PROGRESS.

Building a startup is incredibly hard, especially in the crowded space of consumer social. Despite this, the founders of [Series.so](https://series.so) unveiled themselves from stealth after
raising an astonishing $3M from well-known venture firms and angels. Their core product is their AI social network which users can interface with in iMessage through a
conversational agent. In addition to their network, Series has cultivated hundreds of thousands of eyes on the team through marketing, an
[intern challenge](https://www.linkedin.com/posts/nathaneo-johnson-86aa4a253_introducing-the-series-a-2-week-reality-activity-7344360830127292416-t92R), and
charismatic co-founders who attract college students to use their product.

However, building in public with the data of tens of thousands of college students is a privilege that comes with _major_ responsibilities, especially regarding
application security. In this blog post, I outline two security vulnerabilities I discovered in Series.so's application, the timeline of events, my motivations,
and suggestions for building in the era of AI coding and AI agents.

## The Vulnerabilities

Series' frontend is a NextJs application hosted on Vercel and uses NextAuth to provide Google OAuth sign-in. I began security research after noticing that Series requires you to
provide read access to your Google Calendar to sign up. My calendar is personal, so I am scrutinous of any service that requests this data. From a quick glance of the app (I like poking around), I was particularly concerned
at how I could view my own `user_id` and user data in the Chrome console. Not a good sign of high-quality development practices.

Now with an itch to investigate, I traced the network requests of the application. I noticed that on certian pages of the frontend, images were served to the client from an RSC, but on other pages throughout
the onboarding flow, images were hosted from an external url. This external API ended in `.web.app`, indicating something deployed on Google Cloud Run or using Firebase hosting—likely not just for image hosting. After heading
to the index route of the API, I noticed a response format indicating a FastAPI/Python-style backend. I tested various routes, but I guessed that they used SwaggerUI. Testing the `/openapi.json` route confirmed my suspicions, and then heading to `/docs` displayed their full backend API. This vulnerability required no hacking skills, just some curiosity and a bit of poking around!

The second vulnerability I discovered was after they had patched the first vulnerability (see the [timeline](#appendix---timeline-of-events) below). Any user with an authentication token could query
_any_ user's profile by `user_id`, which included personally identifying information (PII), all of their connections, and shared calendar events.

## Scope of the Vulnerability

If a malicious actor were to have had access to this API, they could have performed a concerning number of actions from the API routes exposed. This includes:

- Determine if a user exists by email or phone number.
- Create a new user.
- Get a user's information by `user_id`, which returns their user object, including PII such as their name, email, phone number,
  age, all users they are connected to, data from their LinkedIn profile, gender, and race (see [concerning practices](#concerning-practices)).
- Since one can search up users by email and users by `user_id`, one could reconstruct Series' social graph through a breadth-first search.
- Upload profile photos to their cloud storage bucket, and modify users' profile photos by `user_id`.

This is incredibly concerning.

## Concerning Practices

There are a variety of actions that Series performs without notifying the user:

- **LinkedIn scraping**: For the route called "augment user", the API scrapes the user's LinkedIn for data.
   ![LinkedIn scraping](/linkedin-scrape.png)
- **Gender and Race detection**: For the route called "analyze face endpoint", the API analyzes the user's profile photo for face detection and metadata extraction. The
   [object for `picture_data`](https://gist.github.com/charliemeyer2000/2114dca872fa1903b349be670c48eb25) contains a field for gender and race, along with a confidence score for each field.
  ![gender and race detection](/analyze-face.png)

Series' privacy policy states that they collect information automatically through [various methods](https://www.series.so/privacy#:~:text=We%20collect%20personal%20information%20from%20you%20in%20the%20following%20ways%3A), but do not state in their app nor in the privacy policy that they detect gender/race nor scrape LinkedIn.

This is a major breach of privacy, and also is illegal in some states! In California, for example, the [California Privacy Rights Act](https://www.caprivacy.org/cpra-exec-summary/) states that businesses must give specific notice and allow you to restrict or opt-out for "Sensitive Personal Information", such as race or biometrics. Yikes.

## Suggestions for improvement

Firstly, authentication is hard; however, this is _not an excuse_ for having unauthenticated endpoints that expose PII. For apps similar to Series performing AI workloads on web applications and
containing sensitive user data, I suggest:

- Use open-source authentication providers like [NextAuth](https://next-auth.js.org/) and [BetterAuth](https://www.better-auth.com/) or managed services like [Clerk](https://clerk.com)
  and [Auth0](https://auth0.com).
- Unless aboslutely necessary, try to keep your application logic tightly coupled to your authentication system. For example, having a NextJs application with NextAuth and then also managing
  authenticating users on another external FastAPI server is _not trivial_. The workloads Series ran would certainly run in a Vercel function, and therefore ensuring only authenticated users can access these endpoints is as easy as an `await auth()` call.
- When exposing endpoints for customer-facing AI agents that display information or take actions on behalf of the user (tool calls, MCPs, chat apps), put strong guardrails in front of your agents. This includes detecting prompt injection, having a robust eval suite, using control theory, and intense observability and tracing of all actions an LLM performs. See [reflections](#reflections) below.

## Reflections

In the era of vibe-coding and AI agents, it's incredibly important to add security layers on LLMs.

For vibe-coded applications, it's enticing to iterate quickly and test an MVP as soon as possible. However, it seems like we're internalizing 
"move fast and break things" too literally; it's too easy to just click "accept all" from Cursor agent. Although AI removes a majority of the drudge work from programming, it does not remove the necessity to think about system design, understand business requirements, and create maintainable and secured systems. I believe AI-generated code should be "untrusted by default," and it is up to the developer to review and test the code.

Secondly, many applications are moving to a new architecture:

![LLM-as-API](/frontend-llm-db.png)

I don't need to explain to you why this is a bad idea. But, if you're putting an MCP in front of your database and have a tool called `execute_sql()`, don't be surprised when you have a [Bobby Tables &trade;](https://xkcd.com/327/) incident. I'm not joking—the Neon MCP server [literally has this](https://github.com/neondatabase-labs/mcp-server-neon/blob/main/src/tools/tools.ts#L85). Your database is only one prompt injection away, if you're not careful! I'd highly suggest adding a layer of abstraction between your MCP/LLM tool calls and your core application logic. Detect prompt injetion with a [classifier](https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2?text=I+like+you.+I+love+you), add an LLM-as-a-judge, obsess over observability and tracing, and eval, eval, eval!

## Conclusion

Thanks to the many people who have supported me through this process of responsible disclosure and writing this report. Your feedback and advice has been invaluable.

At the time of writing this, I have yet to receive a bug bounty from the Series team. They only thing they offered was an insinuation that I should work for them.

---

## Appendix

## Timeline of Events

### Vulnerability 1 - 7/11/25

7/11/25

- **5:22PM:** I began investigating Series.
- **5:35PM:** I demonstrated full access to their backend API. This was publically available on the internet, and finding it was a matter of following network requests. No additional tools or techniques
  were used.
- **5:41-5:58PM:** I reached out via LinkedIn to one of the co-fouders, and on X to another team member.
- **6:54PM:** After no response through social media, I contacted the founders over email.
- **7:00PM:** First contact with the team over email.
- **7:06PM:** I called the team, responsibly disclosed the vulnerability, and suggested improvements to secure their application. I was told I will be contacted by the team once fixed.
- **~10:00PM:** Vulnerability fixed. This was not communicated to me.

### Vulnerability 2 - 7/12/25-26

7/12/25

- **3:12PM:** I verified that the original vulnerability is still fixed.
- **5:18PM:** Found a new vulnerability of an exposed endpoint. Although requiring authentication, it would allow any user to query any other user's data.
- **5:23PM:** I reached out via email again to the founders.
- **6:15PM:** After no response, I texted a co-founder.
- **6:24PM:** I called a co-founder, responsibly disclosed the other vulnerability, and suggested improvements. I was told I will be contacted by the team once fixed.

7/13/25

- **3:54PM**: I had heard no response from the team, checked if the vulnerability was fixed, and it was not. I then reached out via text again.
- **5:47PM**: I recieved a response from the team stating they have been working all day and are almost done.

_incomplete - they haven't fixed this one yet_.


## Screenshots, Proof

- [Here's](https://series-swagger-docs.vercel.app/) a snapshot of the SwaggerUI docs. I didn't download the `openapi.json`, so it's a bit non-functional, this was the main vulnerability from vulnerability 1. I have redacted the actual routes to protect the team.
- For vulnerability 2, I can see shared calendar events of my connections (blurred for privacy) along with all of their data.
  ![calendar events](/calendar-events.png)